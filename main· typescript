import React, { useEffect, useRef, useState } from "react";

// DailyTranslatorFrontend.jsx (UPDATED)
// Enhanced frontend: partial transcripts, latency indicator, mute, transcribe-toggle,
// robust WS reconnect, play-queue and buffering, copy sessionId, small UX improvements.
// Keep backend endpoints as described earlier: /api/daily-token and /ws/audio?sessionId=...

const LANGS = [
  { code: "en", name: "English" },
  { code: "ru", name: "Русский" },
  { code: "es", name: "Español" },
  { code: "fr", name: "Français" },
  { code: "de", name: "Deutsch" },
  { code: "zh", name: "中文" },
  { code: "ja", name: "日本語" },
];

export default function DailyTranslatorFrontend({
  backendBase = "https://api.example.com",
  defaultRoom = "my-test-room",
}) {
  // consent and join state
  const [consented, setConsented] = useState(false);
  const [saveAudioOptIn, setSaveAudioOptIn] = useState(false);
  const [joining, setJoining] = useState(false);
  const [joined, setJoined] = useState(false);

  // routing/state
  const [roomUrl, setRoomUrl] = useState("");
  const [sessionId, setSessionId] = useState("");
  const [status, setStatus] = useState("idle");

  // languages
  const [srcLang, setSrcLang] = useState("auto");
  const [tgtLang, setTgtLang] = useState("en");

  // websocket + audio
  const wsRef = useRef(null);
  const reconnectRef = useRef({ tries: 0, timer: null });
  const audioCtxRef = useRef(null);
  const gainNodeRef = useRef(null);
  const playQueueRef = useRef([]); // queue of AudioBuffer to play sequentially
  const playingRef = useRef(false);

  // UI: transcripts, latency
  const [partials, setPartials] = useState([]); // {text, when, type}
  const [latencyMs, setLatencyMs] = useState(null);
  const [muted, setMuted] = useState(false);
  const [transcribeEnabled, setTranscribeEnabled] = useState(true);
  const bufferThresholdMs = 700; // default buffer before playing (helps avoid choppy playback)

  // Initialize AudioContext + gain
  function ensureAudioCtx() {
    if (!audioCtxRef.current) {
      const AudioContext = window.AudioContext || window.webkitAudioContext;
      audioCtxRef.current = new AudioContext();
      gainNodeRef.current = audioCtxRef.current.createGain();
      gainNodeRef.current.connect(audioCtxRef.current.destination);
      gainNodeRef.current.gain.value = muted ? 0 : 1;
    }
  }

  useEffect(() => {
    if (gainNodeRef.current) gainNodeRef.current.gain.value = muted ? 0 : 1;
  }, [muted]);

  // Play queue processing: sequentially decode/play buffers
  async function enqueueAndPlay(arrayBuffer) {
    ensureAudioCtx();
    const audioCtx = audioCtxRef.current;
    try {
      const decoded = await audioCtx.decodeAudioData(arrayBuffer.slice(0));
      playQueueRef.current.push(decoded);
      if (!playingRef.current) runPlayQueue();
    } catch (e) {
      // fallback: create blob URL and play via audio element
      const blob = new Blob([arrayBuffer], { type: "audio/wav" });
      const url = URL.createObjectURL(blob);
      const a = new Audio(url);
      a.volume = muted ? 0 : 1;
      a.play().catch(err => console.warn("fallback play failed", err));
    }
  }

  async function runPlayQueue() {
    playingRef.current = true;
    const audioCtx = audioCtxRef.current;
    while (playQueueRef.current.length > 0) {
      const buf = playQueueRef.current.shift();
      const src = audioCtx.createBufferSource();
      src.buffer = buf;
      src.connect(gainNodeRef.current);

      // schedule playback immediately, but allow small buffering
      try {
        src.start();
        // wait for buffer duration before continuing to next
        await new Promise(r => setTimeout(r, (buf.duration * 1000) + 20));
      } catch (e) {
        console.warn('start failed', e);
      }
    }
    playingRef.current = false;
  }

  // Robust WebSocket with exponential backoff reconnect
  function connectAudioWs(sid) {
    disconnectAudioWs();
    const wsUrl = backendBase.replace(/^http/, "ws") + `/ws/audio?sessionId=${encodeURIComponent(sid)}`;
    setStatus("ws-connecting");
    try {
      const ws = new WebSocket(wsUrl);
      ws.binaryType = "arraybuffer";
      wsRef.current = ws;

      ws.onopen = () => {
        console.log("WS open");
        setStatus("ws-open");
        reconnectRef.current.tries = 0;
        // send initial settings (languages / transcribe opt)
        sendControl({ type: 'settings', srcLang, tgtLang, transcribe: transcribeEnabled, saveAudio: saveAudioOptIn });
      };

      ws.onmessage = async (ev) => {
        // server may send JSON control messages or binary audio
        if (typeof ev.data === 'string') {
          try {
            const json = JSON.parse(ev.data);
            handleControlMessage(json);
            return;
          } catch (e) {
            // not JSON - ignore
          }
        }
        // binary audio
        if (ev.data instanceof ArrayBuffer || ev.data instanceof Blob) {
          const arrayBuffer = ev.data instanceof Blob ? await ev.data.arrayBuffer() : ev.data;
          await enqueueAndPlay(arrayBuffer);
        }
      };

      ws.onerror = (e) => {
        console.warn('WS error', e);
        setStatus('ws-error');
      };

      ws.onclose = (evt) => {
        console.log('WS closed', evt);
        setStatus('ws-closed');
        scheduleReconnect(sid);
      };

    } catch (e) {
      console.warn('failed(ws)', e);
      setStatus('ws-failed');
      scheduleReconnect(sid);
    }
  }

  function sendControl(obj) {
    try {
      if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
        wsRef.current.send(JSON.stringify(obj));
      }
    } catch (e) { console.warn('send control failed', e); }
  }

  function scheduleReconnect(sid) {
    const r = reconnectRef.current;
    r.tries = Math.min(10, r.tries + 1);
    const delay = Math.min(30000, (500 * Math.pow(2, r.tries))); // exp backoff
    if (r.timer) clearTimeout(r.timer);
    r.timer = setTimeout(() => connectAudioWs(sid), delay);
    setStatus(`reconnect-in-${Math.round(delay/1000)}s`);
  }

  function disconnectAudioWs() {
    if (reconnectRef.current.timer) { clearTimeout(reconnectRef.current.timer); reconnectRef.current.timer = null; }
    if (wsRef.current) {
      try { wsRef.current.close(); } catch (e) {}
      wsRef.current = null;
    }
  }

  function handleControlMessage(json) {
    if (!json || !json.type) return;
    if (json.type === 'partial') {
      // expected {type:'partial', text, serverTimestamp}
      const when = json.serverTimestamp ? new Date(json.serverTimestamp) : new Date();
      setPartials(prev => [{ text: json.text, when, mode: 'partial' }, ...prev].slice(0, 20));
      if (json.serverTimestamp) {
        const now = Date.now();
        setLatencyMs(now - new Date(json.serverTimestamp).getTime());
      }
    }
    if (json.type === 'transcript') {
      // final transcript
      const when = json.serverTimestamp ? new Date(json.serverTimestamp) : new Date();
      setPartials(prev => [{ text: json.text, when, mode: 'final' }, ...prev].slice(0, 50));
    }
    if (json.type === 'info') {
      setStatus(json.msg || 'info');
    }
  }

  // Join room flow
  async function joinRoom() {
    if (!consented) { alert('Подтвердите согласие перед подключением.'); return; }
    setJoining(true);
    setStatus('getting-token');
    try {
      const resp = await fetch(`${backendBase}/api/daily-token?roomId=${encodeURIComponent(defaultRoom)}`);
      if (!resp.ok) throw new Error('token fetch failed');
      const body = await resp.json();
      const sid = body.sessionId || (Math.random().toString(36).slice(2,9));
      setSessionId(sid);
      const token = body.token;
      const rUrl = body.roomUrl || body.room;
      const joinUrl = token ? (rUrl + `?t=${encodeURIComponent(token)}`) : rUrl;
      setRoomUrl(joinUrl);
      // open audio websocket
      connectAudioWs(sid);
      setJoined(true);
      setStatus('joined');
    } catch (err) {
      console.error(err);
      setStatus('join-failed');
      alert('Не удалось получить токен комнаты. Проверьте backend.');
    } finally { setJoining(false); }
  }

  function leaveRoom() {
    setRoomUrl('');
    disconnectAudioWs();
    setJoined(false);
    setStatus('left');
  }

  // UI small helpers
  function copySessionId() {
    if (!sessionId) return;
    navigator.clipboard.writeText(sessionId).then(() => {
      setStatus('session-copied');
      setTimeout(() => setStatus('joined'), 1500);
    });
  }

  function toggleTranscribe(v) {
    setTranscribeEnabled(v);
    sendControl({ type: 'setTranscribe', enabled: v });
  }

  return (
    <div className="min-h-screen bg-gradient-to-b from-slate-900 to-slate-800 text-white p-6">
      <div className="max-w-5xl mx-auto bg-slate-900/60 border border-slate-700 rounded-2xl p-6 shadow-lg">
        <h1 className="text-2xl font-semibold mb-3">Realtime Translator — Frontend (Daily) — Enhanced</h1>
        <p className="text-sm text-slate-400 mb-4">Consent, languages, partial transcripts, low-latency playback via WebSocket. Frontend static — can be hosted on GitHub Pages. Backend required for tokens & streaming.</p>

        <div className="grid md:grid-cols-3 gap-4 mb-4">
          <div className="col-span-2 grid grid-cols-2 gap-3">
            <div>
              <label className="text-xs text-slate-400">Source</label>
              <select value={srcLang} onChange={e => { setSrcLang(e.target.value); sendControl({type:'settings', srcLang:e.target.value}); }} className="mt-1 p-2 rounded-md w-full bg-white/5">
                <option value="auto">Auto-detect</option>
                {LANGS.map(l => <option key={l.code} value={l.code}>{l.name} — {l.code}</option>)}
              </select>
            </div>
            <div>
              <label className="text-xs text-slate-400">Target</label>
              <select value={tgtLang} onChange={e => { setTgtLang(e.target.value); sendControl({type:'settings', tgtLang:e.target.value}); }} className="mt-1 p-2 rounded-md w-full bg-white/5">
                {LANGS.map(l => <option key={l.code} value={l.code}>{l.name} — {l.code}</option>)}
              </select>
            </div>

            <div className="col-span-2 flex items-center gap-3 mt-2">
              <label className="flex items-center gap-2"><input type="checkbox" checked={consented} onChange={e => setConsented(e.target.checked)} /> I consent</label>
              <label className="flex items-center gap-2"><input type="checkbox" checked={saveAudioOptIn} onChange={e => { setSaveAudioOptIn(e.target.checked); sendControl({type:'settings', saveAudio: e.target.checked}); }} /> Save audio (opt-in)</label>
              <label className="flex items-center gap-2"><input type="checkbox" checked={transcribeEnabled} onChange={e => toggleTranscribe(e.target.checked)} /> Transcribe</label>
              <button className="ml-auto px-3 py-1 rounded bg-emerald-500 text-black" onClick={() => { if (!joined) joinRoom(); else leaveRoom(); }}>{joined ? 'Leave' : (joining ? 'Joining...' : 'Join room')}</button>
            </div>
          </div>

          <div className="p-3 rounded bg-slate-800/40 border border-slate-700">
            <div className="text-xs text-slate-400">Status</div>
            <div className="font-mono text-sm mt-1">{status}</div>
            <div className="mt-2 text-xs text-slate-400">Session: <span className="font-mono">{sessionId || '—'}</span></div>
            <div className="mt-2 flex gap-2">
              <button onClick={copySessionId} className="px-2 py-1 rounded bg-slate-700 text-sm">Copy sessionId</button>
              <button onClick={() => setMuted(m => !m)} className="px-2 py-1 rounded bg-slate-700 text-sm">{muted ? 'Unmute' : 'Mute'}</button>
            </div>
            <div className="mt-3 text-xs text-slate-400">Latency: {latencyMs ? `${Math.round(latencyMs)} ms` : '—'}</div>
          </div>
        </div>

        {/* iframe */}
        <div className="rounded-lg overflow-hidden border border-slate-700 bg-black mb-4">
          {roomUrl ? (
            <iframe title="Daily room" src={roomUrl} allow="camera; microphone; autoplay; fullscreen" className="w-full h-[420px]" />
          ) : (
            <div className="p-8 text-center text-slate-400">Room iframe will appear here after you click <strong>Join</strong>.</div>
          )}
        </div>

        {/* Partial transcripts */}
        <div className="grid md:grid-cols-3 gap-4">
          <div className="md:col-span-2 p-4 rounded bg-slate-800 border border-slate-700">
            <div className="text-sm font-medium mb-2">Live transcripts (latest at top)</div>
            <div className="space-y-2 max-h-[220px] overflow-auto p-1">
              {partials.length === 0 ? <div className="text-xs text-slate-400">No transcripts yet.</div> : partials.map((p,i) => (
                <div key={i} className={`p-2 rounded ${p.mode==='final'?'bg-slate-700':'bg-slate-900/30'}`}>
                  <div className="text-xs text-slate-400">{p.when.toLocaleTimeString()}</div>
                  <div className="text-sm">{p.text}</div>
                </div>
              ))}
            </div>
          </div>

          <div className="p-4 rounded bg-slate-800 border border-slate-700">
            <div className="text-sm font-medium mb-2">Playback queue</div>
            <div className="text-xs text-slate-400 mb-2">Queued audio segments: <strong>{playQueueRef.current.length}</strong></div>
            <div className="text-xs text-slate-400">Buffer threshold: {bufferThresholdMs} ms</div>
            <div className="mt-3 text-xs text-slate-400">Notes: Backend must stream TTS audio as binary frames to <code>/ws/audio?sessionId=...</code>. It may also send JSON messages of type "partial" or "transcript" with serverTimestamp to allow latency measurement.</div>
          </div>
        </div>

        <div className="mt-6 text-xs text-slate-400">Deploy notes: Frontend is static — build + ship to GitHub Pages. Backend endpoints must be HTTPS/WSS and accept CORS from your static site origin. Update <code>backendBase</code> prop after deploy.</div>
      </div>
    </div>
  );
}
